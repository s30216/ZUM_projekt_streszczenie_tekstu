{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PCjke0vS7Nx"
   },
   "source": [
    "# Projekt ZUM - Streszczenie tekstu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstępna analiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDp5cu7ZiZ2k",
    "outputId": "fe7acd8d-743d-4681-f399-0b008394ab29"
   },
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aIUh2ND8iocS",
    "outputId": "70848d85-24d8-40dd-a639-7b21f9a9245a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "df_train = ds['train'].to_pandas()\n",
    "df_val = ds['validation'].to_pandas()\n",
    "df_test = ds['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBeU4bnjiezq",
    "outputId": "a3a122bf-53e3-407a-b51c-769456530bde"
   },
   "outputs": [],
   "source": [
    "# wyświetlanie przykładowej pary tekst - streszczenie\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8METFZfyjAfa",
    "outputId": "43c12da3-334a-4bb1-a7f0-f70346996ba9"
   },
   "outputs": [],
   "source": [
    "# średnia długość tekstu i średnia długość streszczeń\n",
    "import numpy as np\n",
    "\n",
    "def words_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "text_len = [words_count(x) for x in df_train[\"article\"]]\n",
    "summary_len = [words_count(x) for x in df_train[\"highlights\"]]\n",
    "\n",
    "print(f\"średnia długość tekstu (wyrazy): {np.mean(text_len)}\")\n",
    "print(f\"średnia długość streszczeń (wyrazy): {np.mean(summary_len)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8d62PIrV_4r",
    "outputId": "852c4579-3e0f-4ab5-9e74-c9dfad6565d0"
   },
   "outputs": [],
   "source": [
    "ratios = [s_len / t_len for s_len, t_len in zip(summary_len, text_len)]\n",
    "print(f\"Średnia proporcja długości streszczeń do tekstów: {np.mean(ratios):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "1oUj3H26UvJ5",
    "outputId": "364e8669-c457-4375-aadf-ec10d6de6459"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogramy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(text_len, bins=50, alpha=0.7, label=\"Długości tekstów\")\n",
    "plt.hist(summary_len, bins=50, alpha=0.7, label=\"Długości streszczeń\")\n",
    "plt.legend()\n",
    "plt.title(\"Histogram długości tekstów i streszczeń\")\n",
    "plt.xlabel(\"Długość\")\n",
    "plt.ylabel(\"Liczba przykładów\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3u-q7GXVklR",
    "outputId": "4658c254-7b02-4851-ce00-8954f6199eee"
   },
   "outputs": [],
   "source": [
    "# Najdłuższy i najkrótszy artykuł\n",
    "longest_article = max(df_train[\"article\"], key=lambda x: words_count(x))\n",
    "shortest_article = min(df_train[\"article\"], key=lambda x: words_count(x))\n",
    "\n",
    "print(f\"Najdłuższy artykuł (liczba słów: {words_count(longest_article)}):\\n{longest_article[:500]}...\\n\")\n",
    "print(f\"Najkrótszy artykuł (liczba słów: {words_count(shortest_article)}):\\n{shortest_article[:500]}\\n\")\n",
    "\n",
    "# Najdłuższe i najkrótsze streszczenie\n",
    "longest_summary = max(df_train[\"highlights\"], key=lambda x: words_count(x))\n",
    "shortest_summary = min(df_train[\"highlights\"], key=lambda x: words_count(x))\n",
    "\n",
    "print(f\"Najdłuższe streszczenie (liczba słów: {words_count(longest_summary)}):\\n{longest_summary[:500]}...\\n\")\n",
    "print(f\"Najkrótsze streszczenie (liczba słów: {words_count(shortest_summary)}):\\n{shortest_summary[:500]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenizacja i liczenie słów\n",
    "df_train['article_word_count'] = df_train['article'].apply(lambda x: words_count(x))\n",
    "df_train['summary_word_count'] = df_train['highlights'].apply(lambda x: words_count(x))\n",
    "\n",
    "# Najczęściej występujące słowa w streszczeniach\n",
    "all_summaries = \" \".join(df_train['highlights'])\n",
    "word_counts = Counter(all_summaries.split())\n",
    "print(word_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inżynieria cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "  text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "  text = text.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  return text\n",
    "\n",
    "train_data = ds['train']\n",
    "train_data = train_data_map = train_data.map(lambda x: {'article': clean_text(x['article']), 'highlights': clean_text(x['highlights'])})\n",
    "val_data = ds['validation'].map(lambda x: {'article': clean_text(x['article']), 'highlights': clean_text(x['highlights'])})\n",
    "test_data = ds['test'].map(lambda x: {'article': clean_text(x['article']), 'highlights': clean_text(x['highlights'])})\n",
    "\n",
    "train_data[:2], val_data[:2], test_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_data(examples):\n",
    "    inputs = [f\"summarize: {article}\" for article in examples['article']]\n",
    "    targets = examples['highlights']\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=150,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "train_data_tokenized = train_data.map(\n",
    "    tokenize_data,\n",
    "    batched=True,\n",
    "    remove_columns=['article', 'highlights'],\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "val_data_tokenized = val_data.map(\n",
    "    tokenize_data,\n",
    "    batched=True,\n",
    "    remove_columns=['article', 'highlights'],\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "test_data_tokenized = test_data.map(\n",
    "    tokenize_data,\n",
    "    batched=True,\n",
    "    remove_columns=['article', 'highlights'],\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "train_data_tokenized = train_data_tokenized.with_format(\"torch\")\n",
    "val_data_tokenized = val_data_tokenized.with_format(\"torch\")\n",
    "test_data_tokenized = test_data_tokenized.with_format(\"torch\")\n",
    "\n",
    "print(train_data_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data_tokenized,\n",
    "    eval_dataset=val_data_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(eval_dataset=val_data_tokenized)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.evaluate(eval_dataset=test_data_tokenized)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for the test dataset\n",
    "test_sample = test_data_tokenized[0]\n",
    "input_ids = test_sample['input_ids']\n",
    "outputs = model.generate(input_ids=input_ids.unsqueeze(0), max_length=150)\n",
    "\n",
    "# Decode predictions\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
